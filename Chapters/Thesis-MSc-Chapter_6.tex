% #############################################################################
% This is Chapter 6
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{System Evaluation}
\cleardoublepage
% The following line allows to ref this chapter
\label{chap:evaluation}

This chapter concerns the system and board evaluation. The system and board were evaluated regarding its performance and the fullfilled requirements outlined in the previous project report and the problem definition chapter \ref{chap:problem}.
% -----------------------------------------------------
% -----------------------------------------------------
\section{Performance Tests}\label{chap:evaluation:performance}

The test objectives, configuration, results and conclusions are detailed for every tested component.
The communication channel, smartfusion2 board's security services and implemented services were tested.
Two performance metrics were calculated, the test processing time, and the tested component's throughput.

% -----------------------------------------------------
\subsection{Testing configuration}\label{chap:evaluation:performance:config}

The tests were all performed on a Windows 10 computer, connected to the smartfusion2 device through a \ac{UART} serial port. The implemented PKCS\#11 program interface was used to run the tests.
Two programs were running on the computer while performing the tests, the PKCS\#11 interface program and the SoftConsole IDE to run the code on the smartfusion board.
For all tests, the serial port UART connection was configured with a 115200 bit/s baud rate.%, 8 data bits, no parity bits and one stop bit.
Since the board does not provide a clock and \ac{API} to measure elapsed time, the time has to be measured in the interface software side.
The elapsed time was measured using the function \texttt{gettimeofday()} available in the C library \texttt{sys/time.h}.
Time measurement starts right before sending a message to the device which triggers the operation, and stops when the client receives a message from the device, after the operation is finished.
In order to thoroughly study the performance and scalability of each component, the transmitted data size was varied, only for components where it is logical and can potentially have a performance impact. The data size was tested, when possible, up to 36 KBytes. The size is limited by the device's \ac{RAM} of 64 KB.
Tests were performed in two different configurations.
Obvious outlier values were excluded from the experimental calculations. The adopted rule was that values which are significantly above or below the average, and are never repeated were eliminated from the sample set.
For the first test configuration, the measured operation is performed once in the device each transmission. This transmission is repeated multiple times, minimum thirty, for each set of values, until an acceptable variance is achieved. For most components the variance is well bellow 1\%. % The more volatile test results have a variance below 4\%.
For most components the first configuration produced unstable results with a high variance, due to the communications overhead on every test run.
Thus, a second test configuration was applied on components where the time to transmit messages needs to be minimized to more accurately assess its performance. For each transmission, the operation was performed 100 consecutive times in the board. The resulting time was divided by 100 to obtain the average processing time of the operation.
With the second configuration, the tests are visibly more stable.
A test example of the difference between both configurations results was on testing the \ac{AES} security service. The first scenario's communication overhead was on average 15\% compared to the second configuration results, with a peak 61\% overhead for small data sizes. Additionally when plotting the results, the second results scale almost perfectly linear, and the first results' are significantly more unstable.

% -----------------------------------------------------
\subsection{Communications}\label{chap:evaluation:performance:comms}

In order to assess the communication channel performance, and its impact on the system, the average time to transmit data was measured. For each test, the interface sent some data of a specific size to the device, and it returned an acknowledge message on reception. The test, in accordance with the first scenario, was repeated at least thirty times for each data size. The highest variance did not go above 0.2\%.
The average transmission times for each value are displayed in figure \ref{fig:comms:time}.
The values range from 0.048 seconds for 0.5 KB, to 3.2 s for 36 KB.
From the graph we can conclude the performance has linear scalability.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{./Images/comms-time.png}
	\caption{Serial Port data transmission times}
	\label{fig:comms:time}
\end{figure}

For the subsequent graphic, the throughput was calculated from the transmission tests, for every repetition.
Figure~\ref{fig:comms:tput} plots the experimental throughput and theoretical throughput. The theoretical throughput was calculated from the baud rate \(115200/8 = 14.06 KBytes/s\).
We observe the experimental throughput starts at around 10 KB/s for smaller data sizes and stabilizes around 11 KB/s as data size increases.
We can conclude the practical throughput is close to the theoretical, and as expected stabilizes as the data size increases.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{./Images/comms-tput.png}
	\caption{Serial port data transmission throughput}
	\label{fig:comms:tput}
\end{figure}

% -----------------------------------------------------
% -----------------------------------------------------
\subsection{Smartfusion2 Security Services}\label{chap:evaluation:board}

All the security core accelerators of the smartfusion2 SoC were tested. This includes \ac{AES}, \ac{SHA}, \ac{HMAC}, KeyTree based \ac{SHA} and \ac{ECC} scalar multiplication and point addition.
As discussed before, all services were tested using the two presented configurations. The results of the second configuration are significantly more stable and will are presented next.
Twenty different data sizes were used in the test, from 0.5 KB to 36 KB.

The services were tested taking into account three different time components \(T_{total} = T_{call} + T_{transmittion} + T_{service}\), with the first test configuration. The call fraction is the time it takes calling the PKCS\#11 API, before any data is transmitted or the service is executed. From the results, it can be concluded this component's impact on performance is negligible, since it is almost always 0.
Predictably, the transmission time always follow the performance model studied in the previous section.
Thus, the following security core and implemented services tests focused on solely measuring the performance of the service. To get a complete conclusion of the performance of the overall system, the total service processing time can be added to the serial port time.
Regarding the time test results, all services followed a near perfect linear evolution in function of the processed data size. Thus, each accelerator's performance can be modeled with a formula composed of two different components \ref{eq:linear-eq}, a factor independent of the data being processed, present in every service call, and a component dependent on the data size.

\begin{equation}
	\label{eq:linear-eq}
	T_{total} = T_{constant} + T_{data} * KBytes
\end{equation}
Linear regression was used to calculate these values, presented in table \ref{tab:core-model}. The median average percentage error was calculated to assess the accuracy of the calculated models for representing each service's performance. Services which receive a fixed data size such as, the \ac{ECC} and KeyTree services, only have a constant time component.

\input{tables_and_code/cores-model.tex}

All time values are presented in milliseconds, the median average percentage error represents the error of the model values compared to the real results.
The \ac{AES} service was tested with all possible variations. Namely, with 128 bit and 256 bit keys, with all four available modes, with encryption and decryption. Only one result is shown since there was no variation in all results. The chosen \ac{AES} mode, key size or encryption/decryption operation does not impact the performance positively or negatively.
Regarding the results, the higher time factor dependent of the data implicates the service time increases faster. Overall the \ac{AES} and \ac{HMAC} total service time increases faster due to this compared to \ac{SHA}, which increases significantly slower. The constant time is similar among the three services with \ac{HMAC}'s a little higher.
The error percentages are all bellow 1 \%, proving the calculated models accurately represent all three services' performance.
The \ac{ECC} services are the slowest performers, particularly the scalar multiplication, each run taking more than half a second.

% TODO
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\textwidth]{./Images/core-tput.png}
	\caption{TODO}
	\label{fig:performance:core-tput}
\end{figure}

This graphic represents the throughput calculated from the processing time results previously gathered. The graphic shows all services throughput, shown in KB/ms, increases as the data size also increases, all eventually stabilizing at a specific value. \ac{AES} stabilizes at around 1.2 KB/ms, and both \ac{SHA} and \ac{HMAC} at around 0.1 KB/ms.

% -----------------------------------------------------
% -----------------------------------------------------
\subsection{Implemented Services}\label{chap:evaluation:services}

\input{tables_and_code/services-model.tex}

% TODO
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\textwidth]{./Images/services-tput.png}
	\caption{TODO}
	\label{fig:performance:services-tput}
\end{figure}

% TODO
The results in table~ for the encryption and decryption operations are very similar due to both using the same board services, AES encryption and HMAC, but in a different order. It is also important to note AES encryption and decryption in CTR mode is essentially the same operation due the mode's characteristics.
Relating to the variation in data size, the values vary between approximately 0.1284 and 0.1825 seconds, which is a very insignificant difference. Thus we can conclude, the data size has a negligible impact on the operations performance.

% TODO
Regarding the key generation operation results in table~, two values were obtained through different methods. Due to the operation using SRAM-PUF services to enroll new keys in the eNVM memory, with limited write cycles and key slots, this operation cannot be repeated enough times to get a relevant enough sample size.
So a trade off was achieved. The operation was performed 1000 times without the key enrollment operation, meaning only the ecdh key generation algorithm and key derivation function (SHA-256).
Since the enrollment phase is presumed to be expensive, due to writing in eNVM memory, the test was also performed 10 times with key enrollment, to get an idea of its potential performance cost.

A higher time of 0.577 seconds without enrollment and 1.764s with, compared to the previous operations is expected due to the higher cost of operations with asymmetric keys.
However, comparing both values we can conclude saving the key in memory, has most likely the higher performance impact on the operation.
This result is congruent with the one obtained by \cite{parrinha2017flexible} of 0.57s per ECC scalar multiplication.
Thus we can conclude, the scalar multiplication is the second most expensive operation, after the key enrollment, since the key derivation function has a negligible impact on the processing time (0.577s with the function vs 0.57s without).

% -----------------------------------------------------
% -----------------------------------------------------
\section{Requirements}\label{chap:evaluation:requirements}

A M2S090TS smartfusion2 evaluation kit is priced at 384â‚¬ \cite{smartfusionPrice}.

% ------------- Requirements ------------------
% Devices should be distributable to individuals or entities with one or more individuals;
% The system must allow communications between individuals representing themselves or an entity;
% The system must be responsible for securing all communications against any sort of attacks;
% The device should be independent from user's personal computers;
% Users should be able to create secure communications with available and new entities;
% m New secure connections should be created, if existing communications are suspected to be compromised;
% It should provide an easy-to-use interface by everyone, including non-technical people;
% It should have a relatively low cost, to allow distribution of several devices among multiple people;
% Only authorized individuals should be able to use the device.
% ---------------------------------------------

% secure comms - aes and hmac services are not dpa resistant, so keys should be regularly replaced to avoid enough data which enables an attacker to break encryption. On the other hand this is only possible if the attacker has physical access to the device or potentially with some type of malware on the user's computer.
% key generation - needs assymetric keys to generate new keys with public keys and salt traded beforehand - can be not user friendly

% -----------------------------------------------------
% -----------------------------------------------------
\section{Summary}\label{chap:evaluation:summary}
